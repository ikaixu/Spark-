# Data Camp 学习教程
You might already know Apache Spark as a fast and general engine for big data processing, with built-in modules for streaming, SQL, machine learning and graph processing. It’s well-known for its speed, ease of use, generality and the ability to run virtually everywhere. And even though Spark is one of the most asked tools for data engineers, also data scientists can benefit from Spark when doing exploratory data analysis, feature extraction, supervised learning and model evaluation.  

Today’s post will introduce you to some basic Spark in Python topics, based on 9 of the most frequently asked questions, such as  
本文谈一谈一些用Spark的基础知识，9个方面的问题  

+ What language to pick when you’re working with Spark: Python or Scala? What are the benefits of using one over the other?  
将python与scala对比，在spark领域孰优孰劣？  

+ Next, you’ll see how you can work with Spark in Python: locally or via the Jupyter Notebook. You’ll learn how to install Spark and how to run Spark applications with Jupyter notebooks, either by adding PySpark as any other library, by working with a kernel or by running PySpark with Jupyter in Docker containers.  
接下来，你将学会在Jupyter Notebook条件下写Spark程序。  

+ Now that you have made sure that you can work with Spark in Python, you’ll get to know one of the basic building blocks that you will frequently use when you’re working with PySpark: the RDD. You’ll learn how the RDD differs from the DataFrame API and the DataSet API and when you should use which structure.  
RDD组件，RDD与DataSet、DataFrame的区别  

+ Then, you’ll learn more about the differences between Spark DataFrames and Pandas DataFrames and how you can switch from one to the other. If you want to go further into Pandas DataFrames, consider DataCamp’s Pandas Foundations course.  
学习pandas dataframe和spark dataframe的区别  

+ When you’re working with RDDs, it’s very important to understand the differences between RDD actions and transformations,  
RDD动作与转换  

+ And why you need to cache or persist RDDs, or when you need to broadcast a variable.  
保持RDD  

+ To round up, you’ll get introduced to some of the best practices in Spark, like using DataFrames and the Spark UI,  
实践环节  

+ And you’ll also see how you can turn off the logging for PySpark.

## A1.Spark : scala or python?
+ In general, most developers seem to agree that Scala wins in terms of performance and concurrency.  

+ In short, the above explains why it’s still strongly recommended to use Scala over Python when you’re working with streaming data, even though structured streaming in Spark seems to reduce the gap already.  

+ And, lastly, there are some advanced features that might sway you to use either Python or Scala. Here, you would have to argue that Python has the main advantage if you’re talking about data science, as it provides the user with a lot of great tools for machine learning and natural language processing, such as SparkMLib.

## A2.How To Install Spark?
1. Installing Spark and getting to work with it can be a daunting task. This section will go deeper into how you can install it and what your options are to start working with it.  

2. First, check if you have the Java jdk installed. Then, go to the Spark download page. Keep the default options in the first three steps and you’ll find a downloadable link in step 4. Click to download it.  

3. Next, make sure that you untar the directory that appears in your “Downloads” folder. Next, move the untarred folder to /usr/local/spark  

4. Now that you’re all set to go, open the README file in /usr/local/spark. You’ll see that you’ll need to run a command to build Spark if you have a version that has not been built yet. So, make sure you run the command.This might take a while, but after this, you’re all set to go!  

### A2.1 与pyspark交互
Next, you can immediately start working in the Spark shell by typing ./bin/pyspark in the same folder in which you left off at the end of the last section. It can take a bit of time, but eventually, you’ll see something like this:You’re ready to start working interactively!  

`Note` that the SparkContext has already been initialized. You don’t need to import SparkContext from pyspark to begin working.  

Of course, you can adjust the command to start the Spark shell according to the options that you want to change. In the following command, you see that the --master argument allows you to specify to which master the SparkContext connects to. In this case, you see that the local mode is activated. The number in between the brackets designates the number of cores that are being used; In this case, you use all cores, while local[4] would only make use of four cores.  

`Note` that the application UI is available at localhost:4040. Open up a browser, paste in this location and you’ll get to see a dashboard with tabs designating jobs, stages, storage, etc. This will definitely come in handy when you’re executing jobs and looking to tune them. You’ll read more about this further on.  

### A2.2 Running Spark Applications Using Jupyter Notebooks
